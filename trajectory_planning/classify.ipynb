{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['feature', 'label'])\n",
      "tensor([[ -1.9408,   4.2829,  -7.0747,  ...,  17.2657,  -2.8689,   4.2029],\n",
      "        [ -1.2524,   3.3057,  -1.6193,  ...,   9.4712, -10.3804,  -3.0191],\n",
      "        [  1.1509,  -2.9682,  -9.6701,  ...,  -3.4445,   6.2805,  -0.3120],\n",
      "        ...,\n",
      "        [ -1.5408,   2.5173,  -1.8167,  ...,  12.3208,  -1.4063,  -0.0334],\n",
      "        [ -4.4179,   2.0405, -12.9277,  ...,  12.6393,  -2.1744,  -3.5770],\n",
      "        [ -3.1111,  -2.0232,  -7.9568,  ...,   7.1998,  -6.0398,   3.6039]])\n",
      "tensor([5, 0, 4,  ..., 5, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch  # 命令行是逐行立即执行的\n",
    "content = torch.load('data.pth')\n",
    "print(content.keys())   # keys()\n",
    "# 之后有其他需求比如要看 key 为 model 的内容有啥\n",
    "print(content['feature'])\n",
    "print(content['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -1.9408,   4.2829,  -7.0747,  ...,  17.2657,  -2.8689,   4.2029],\n",
      "        [ -1.2524,   3.3057,  -1.6193,  ...,   9.4712, -10.3804,  -3.0191],\n",
      "        [  1.1509,  -2.9682,  -9.6701,  ...,  -3.4445,   6.2805,  -0.3120],\n",
      "        [  8.5502,   4.3298, -11.2310,  ...,  -2.5779,  -7.6427,   0.9930],\n",
      "        [  2.4465,  -2.8796,  -1.0813,  ...,  12.2426,  -7.7521, -14.5718]]) tensor([5, 0, 4, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the hyperparameters for data creation\n",
    "NUM_CLASSES = 10\n",
    "NUM_FEATURES = 256\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# 1. Create multi-class data\n",
    "# X_blob, y_blob = make_blobs(n_samples=1000,\n",
    "#     n_features=NUM_FEATURES, # X features\n",
    "#     centers=NUM_CLASSES, # y labels\n",
    "#     cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n",
    "#     random_state=RANDOM_SEED\n",
    "# )\n",
    "#\n",
    "# # 2. Turn data into tensors\n",
    "# X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "# y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
    "X_blob=content['feature']\n",
    "y_blob=content['label']\n",
    "\n",
    "print(X_blob[:5], y_blob[:5])\n",
    "\n",
    "# 3. Split into train and test sets\n",
    "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n",
    "    y_blob,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# 4. Plot data\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'cpu'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Make device agnostic code\n",
    "device =  \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "BlobModel(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_layer_stack): Sequential(\n    (0): Linear(in_features=256, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=64, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=10, bias=True)\n  )\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Build model\n",
    "class BlobModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n",
    "\n",
    "        Args:\n",
    "            input_features (int): Number of input features to the model.\n",
    "            out_features (int): Number of output features of the model\n",
    "              (how many classes there are).\n",
    "            hidden_units (int): Number of hidden units between layers, default 8.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=64, out_features=10), # how many classes are there?\n",
    "\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.flatten(x)\n",
    "        logists=self.linear_layer_stack(x)\n",
    "        return logists\n",
    "\n",
    "# Create an instance of BlobModel and send it to the target device\n",
    "model_4 = BlobModel().to(device)\n",
    "model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_4.parameters(),lr=0.01,momentum=0.9) # exercise: try changing the learning rate here and seeing what happens to the model's performance\n",
    "# optimizer=torch.optim.Adam(params=model_4.parameters(),lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(model_4(X_blob_train.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.00492, Acc: 99.99% | Test Loss: 0.11910, Test Acc: 97.17%\n",
      "Epoch: 10 | Loss: 0.00488, Acc: 99.99% | Test Loss: 0.11921, Test Acc: 97.17%\n",
      "Epoch: 20 | Loss: 0.00483, Acc: 99.99% | Test Loss: 0.11933, Test Acc: 97.17%\n",
      "Epoch: 30 | Loss: 0.00479, Acc: 99.99% | Test Loss: 0.11945, Test Acc: 97.17%\n",
      "Epoch: 40 | Loss: 0.00475, Acc: 99.99% | Test Loss: 0.11956, Test Acc: 97.18%\n",
      "Epoch: 50 | Loss: 0.00470, Acc: 99.99% | Test Loss: 0.11968, Test Acc: 97.18%\n",
      "Epoch: 60 | Loss: 0.00466, Acc: 99.99% | Test Loss: 0.11980, Test Acc: 97.18%\n",
      "Epoch: 70 | Loss: 0.00462, Acc: 99.99% | Test Loss: 0.11991, Test Acc: 97.18%\n",
      "Epoch: 80 | Loss: 0.00458, Acc: 99.99% | Test Loss: 0.12003, Test Acc: 97.17%\n",
      "Epoch: 90 | Loss: 0.00454, Acc: 99.99% | Test Loss: 0.12014, Test Acc: 97.17%\n",
      "Epoch: 100 | Loss: 0.00450, Acc: 99.99% | Test Loss: 0.12026, Test Acc: 97.17%\n",
      "Epoch: 110 | Loss: 0.00446, Acc: 99.99% | Test Loss: 0.12037, Test Acc: 97.17%\n",
      "Epoch: 120 | Loss: 0.00442, Acc: 99.99% | Test Loss: 0.12049, Test Acc: 97.18%\n",
      "Epoch: 130 | Loss: 0.00438, Acc: 99.99% | Test Loss: 0.12060, Test Acc: 97.18%\n",
      "Epoch: 140 | Loss: 0.00435, Acc: 99.99% | Test Loss: 0.12071, Test Acc: 97.18%\n",
      "Epoch: 150 | Loss: 0.00431, Acc: 99.99% | Test Loss: 0.12083, Test Acc: 97.20%\n",
      "Epoch: 160 | Loss: 0.00427, Acc: 99.99% | Test Loss: 0.12094, Test Acc: 97.20%\n",
      "Epoch: 170 | Loss: 0.00423, Acc: 99.99% | Test Loss: 0.12105, Test Acc: 97.20%\n",
      "Epoch: 180 | Loss: 0.00420, Acc: 99.99% | Test Loss: 0.12116, Test Acc: 97.21%\n",
      "Epoch: 190 | Loss: 0.00416, Acc: 99.99% | Test Loss: 0.12127, Test Acc: 97.21%\n",
      "Epoch: 200 | Loss: 0.00413, Acc: 99.99% | Test Loss: 0.12139, Test Acc: 97.21%\n",
      "Epoch: 210 | Loss: 0.00409, Acc: 99.99% | Test Loss: 0.12150, Test Acc: 97.21%\n",
      "Epoch: 220 | Loss: 0.00406, Acc: 100.00% | Test Loss: 0.12160, Test Acc: 97.21%\n",
      "Epoch: 230 | Loss: 0.00403, Acc: 100.00% | Test Loss: 0.12171, Test Acc: 97.20%\n",
      "Epoch: 240 | Loss: 0.00399, Acc: 100.00% | Test Loss: 0.12182, Test Acc: 97.20%\n",
      "Epoch: 250 | Loss: 0.00396, Acc: 100.00% | Test Loss: 0.12193, Test Acc: 97.20%\n",
      "Epoch: 260 | Loss: 0.00393, Acc: 100.00% | Test Loss: 0.12204, Test Acc: 97.20%\n",
      "Epoch: 270 | Loss: 0.00390, Acc: 100.00% | Test Loss: 0.12214, Test Acc: 97.20%\n",
      "Epoch: 280 | Loss: 0.00387, Acc: 100.00% | Test Loss: 0.12225, Test Acc: 97.20%\n",
      "Epoch: 290 | Loss: 0.00384, Acc: 100.00% | Test Loss: 0.12236, Test Acc: 97.20%\n",
      "Epoch: 300 | Loss: 0.00381, Acc: 100.00% | Test Loss: 0.12247, Test Acc: 97.20%\n",
      "Epoch: 310 | Loss: 0.00378, Acc: 100.00% | Test Loss: 0.12258, Test Acc: 97.20%\n",
      "Epoch: 320 | Loss: 0.00375, Acc: 100.00% | Test Loss: 0.12268, Test Acc: 97.20%\n",
      "Epoch: 330 | Loss: 0.00372, Acc: 100.00% | Test Loss: 0.12279, Test Acc: 97.18%\n",
      "Epoch: 340 | Loss: 0.00369, Acc: 100.00% | Test Loss: 0.12289, Test Acc: 97.18%\n",
      "Epoch: 350 | Loss: 0.00366, Acc: 100.00% | Test Loss: 0.12300, Test Acc: 97.19%\n",
      "Epoch: 360 | Loss: 0.00363, Acc: 100.00% | Test Loss: 0.12310, Test Acc: 97.18%\n",
      "Epoch: 370 | Loss: 0.00360, Acc: 100.00% | Test Loss: 0.12321, Test Acc: 97.18%\n",
      "Epoch: 380 | Loss: 0.00358, Acc: 100.00% | Test Loss: 0.12331, Test Acc: 97.18%\n",
      "Epoch: 390 | Loss: 0.00355, Acc: 100.00% | Test Loss: 0.12342, Test Acc: 97.18%\n",
      "Epoch: 400 | Loss: 0.00352, Acc: 100.00% | Test Loss: 0.12352, Test Acc: 97.18%\n",
      "Epoch: 410 | Loss: 0.00350, Acc: 100.00% | Test Loss: 0.12362, Test Acc: 97.17%\n",
      "Epoch: 420 | Loss: 0.00347, Acc: 100.00% | Test Loss: 0.12372, Test Acc: 97.17%\n",
      "Epoch: 430 | Loss: 0.00344, Acc: 100.00% | Test Loss: 0.12382, Test Acc: 97.17%\n",
      "Epoch: 440 | Loss: 0.00342, Acc: 100.00% | Test Loss: 0.12393, Test Acc: 97.17%\n",
      "Epoch: 450 | Loss: 0.00339, Acc: 100.00% | Test Loss: 0.12403, Test Acc: 97.17%\n",
      "Epoch: 460 | Loss: 0.00337, Acc: 100.00% | Test Loss: 0.12413, Test Acc: 97.18%\n",
      "Epoch: 470 | Loss: 0.00334, Acc: 100.00% | Test Loss: 0.12423, Test Acc: 97.18%\n",
      "Epoch: 480 | Loss: 0.00332, Acc: 100.00% | Test Loss: 0.12433, Test Acc: 97.18%\n",
      "Epoch: 490 | Loss: 0.00329, Acc: 100.00% | Test Loss: 0.12443, Test Acc: 97.18%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 500\n",
    "\n",
    "# Put data to target device\n",
    "X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n",
    "X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_4.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "\n",
    "    y_logits = model_4(X_blob_train) # model outputs raw logits\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "    # print(y_logits)\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_blob_train)\n",
    "    acc = accuracy_fn(y_true=y_blob_train,\n",
    "                      y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_4.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass\n",
    "      test_logits = model_4(X_blob_test)\n",
    "      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "      # 2. Calculate test loss and accuracy\n",
    "      test_loss = loss_fn(test_logits, y_blob_test)\n",
    "      test_acc = accuracy_fn(y_true=y_blob_test,\n",
    "                             y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model_4.state_dict(), \"D:\\\\files2\\\\ai\\\\lab\\\\AiLabCode\\\\Proj3\\\\model8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model_4, \"D:\\\\files2\\\\ai\\\\lab\\\\AiLabCode\\\\Proj3\\\\model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "BlobModel(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_layer_stack): Sequential(\n    (0): Linear(in_features=256, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=64, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=10, bias=True)\n  )\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFinal=torch.load(\"D:\\\\files2\\\\ai\\\\lab\\\\AiLabCode\\\\Proj3\\\\model2.pth\")\n",
    "modelFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "torch.save(modelFinal.state_dict(), \"D:\\\\files2\\\\ai\\\\lab\\\\AiLabCode\\\\Proj3\\\\model7.pth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict=modelFinal.load_state_dict(torch.load(\"D:\\\\files2\\\\ai\\\\lab\\\\AiLabCode\\\\Proj3\\\\model7.pth\"))\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.00241, Acc: 100.00% | Test Loss: 0.12751, Test Acc: 97.19%\n",
      "Epoch: 10 | Loss: 0.00241, Acc: 100.00% | Test Loss: 0.12751, Test Acc: 97.19%\n",
      "Epoch: 20 | Loss: 0.00241, Acc: 100.00% | Test Loss: 0.12751, Test Acc: 97.19%\n",
      "Epoch: 30 | Loss: 0.00241, Acc: 100.00% | Test Loss: 0.12751, Test Acc: 97.19%\n",
      "Epoch: 40 | Loss: 0.00241, Acc: 100.00% | Test Loss: 0.12751, Test Acc: 97.19%\n",
      "Epoch: 50 | Loss: 0.00241, Acc: 100.00% | Test Loss: 0.12751, Test Acc: 97.19%\n",
      "Epoch: 60 | Loss: 0.00241, Acc: 100.00% | Test Loss: 0.12751, Test Acc: 97.19%\n",
      "Epoch: 70 | Loss: 0.00241, Acc: 100.00% | Test Loss: 0.12751, Test Acc: 97.19%\n",
      "Epoch: 80 | Loss: 0.00241, Acc: 100.00% | Test Loss: 0.12751, Test Acc: 97.19%\n",
      "Epoch: 90 | Loss: 0.00241, Acc: 100.00% | Test Loss: 0.12751, Test Acc: 97.19%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n",
    "X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    modelFinal.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "\n",
    "    y_logits = modelFinal(X_blob_train) # model outputs raw logits\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "    # print(y_logits)\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_blob_train)\n",
    "    acc = accuracy_fn(y_true=y_blob_train,\n",
    "                      y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    modelFinal.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass\n",
    "      test_logits = modelFinal(X_blob_test)\n",
    "      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "      # 2. Calculate test loss and accuracy\n",
    "      test_loss = loss_fn(test_logits, y_blob_test)\n",
    "      test_acc = accuracy_fn(y_true=y_blob_test,\n",
    "                             y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7905, -0.9058, -0.2607, -0.5465,  2.1174], requires_grad=True)\n",
      "tensor([[-1.7118,  0.1651,  1.5819,  0.4485,  0.0330],\n",
      "        [ 1.4503, -0.6936,  0.9967,  0.6131,  0.7764],\n",
      "        [-0.3029, -1.2753, -0.4758,  2.3839,  0.9157]])\n",
      "tensor([0.6160, 0.5309, 2.1548], grad_fn=<ReluBackward0>)\n",
      "tensor([0.6160, 0.5309, 2.1548], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# NB: vmap is only available on nightly builds of PyTorch.\n",
    "# You can download one at pytorch.org if you're interested in testing it out.\n",
    "from functorch import vmap\n",
    "batch_size, feature_size = 3, 5\n",
    "weights = torch.randn(feature_size, requires_grad=True)\n",
    "\n",
    "# Note that model doesn't work with a batch of feature vectors because\n",
    "# torch.dot must take 1D tensors. It's pretty easy to rewrite this\n",
    "# to use `torch.matmul` instead, but if we didn't want to do that or if\n",
    "# the code is more complicated (e.g., does some advanced indexing\n",
    "# shenanigins), we can simply call `vmap`. `vmap` batches over ALL\n",
    "# inputs, unless otherwise specified (with the in_dims argument,\n",
    "# please see the documentation for more details).\n",
    "def model(feature_vec):\n",
    "    # Very simple linear model with activation\n",
    "    return feature_vec.dot(weights).relu()\n",
    "\n",
    "examples = torch.randn(batch_size, feature_size)\n",
    "result = vmap(model)(examples)\n",
    "expected = torch.stack([model(example) for example in examples.unbind()])\n",
    "print(weights)\n",
    "print(examples)\n",
    "print(result)\n",
    "print(expected)\n",
    "\n",
    "assert torch.allclose(result, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     11\u001B[0m     n_targets \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m40\u001B[39m\n\u001B[1;32m---> 12\u001B[0m     agent \u001B[38;5;241m=\u001B[39m \u001B[43mAgent\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;66;03m# This is a example of what the evaluation procedure looks like.\u001B[39;00m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m# The whole dataset is divided into a training set and a test set.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# The training set (including `data` and `label`) is distributed to you.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;66;03m# But in the final evaluation we will use the test set.\u001B[39;00m\n\u001B[0;32m     18\u001B[0m     data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\files2\\ai\\lab\\AiLabCode\\Proj3\\project3\\agent.py:22\u001B[0m, in \u001B[0;36mAgent.__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# TODO: prepare your agent here\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# self.model = torch.load(os.path.abspath('./model2.pth'))\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mfiles2\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mai\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mlab\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mAiLabCode\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mProj3\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mproject3\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mmodel2.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_dict \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_4\u001B[49m\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mfiles2\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mai\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mlab\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mAiLabCode\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mProj3\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mmodel5.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magentScores \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model_4' is not defined"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6ee7413fb220e3fb574f1307f78dc400d698e7a347651c41404bcdfa4682e4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
